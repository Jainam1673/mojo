"""
Example: Neural Network Building Blocks
Demonstrates how MojoTensor can be used for machine learning.
"""

from src.core.tensor import Tensor
from src.ops.elementwise import add, mul, scalar_mul, exp, scalar_add
from src.ops.linalg import matmul, dot
from src.ops.reduction import sum, mean

# =============================================================================
# Activation Functions
# =============================================================================

fn relu[dtype: DType](x: Tensor[dtype]) -> Tensor[dtype]:
    """ReLU activation: max(0, x)"""
    var result = Tensor[dtype](x.shape)
    
    for i in range(x.size):
        if x.data[i] > 0:
            result.data[i] = x.data[i]
        else:
            result.data[i] = 0
    
    return result

fn sigmoid[dtype: DType](x: Tensor[dtype]) -> Tensor[dtype]:
    """Sigmoid activation: 1 / (1 + e^(-x))"""
    var neg_x = scalar_mul(x, -1.0)
    var exp_neg_x = exp(neg_x)
    var one_plus_exp = scalar_add(exp_neg_x, 1.0)
    
    var result = Tensor[dtype](x.shape)
    for i in range(x.size):
        result.data[i] = 1.0 / one_plus_exp.data[i]
    
    return result

fn tanh[dtype: DType](x: Tensor[dtype]) -> Tensor[dtype]:
    """Tanh activation: (e^x - e^(-x)) / (e^x + e^(-x))"""
    var exp_x = exp(x)
    var neg_x = scalar_mul(x, -1.0)
    var exp_neg_x = exp(neg_x)
    
    var result = Tensor[dtype](x.shape)
    for i in range(x.size):
        var numerator = exp_x.data[i] - exp_neg_x.data[i]
        var denominator = exp_x.data[i] + exp_neg_x.data[i]
        result.data[i] = numerator / denominator
    
    return result

fn softmax[dtype: DType](x: Tensor[dtype]) -> Tensor[dtype]:
    """Softmax: e^x / sum(e^x)"""
    var exp_x = exp(x)
    var sum_exp = sum(exp_x)
    
    var result = Tensor[dtype](x.shape)
    for i in range(x.size):
        result.data[i] = exp_x.data[i] / sum_exp
    
    return result

# =============================================================================
# Loss Functions
# =============================================================================

fn mse_loss[dtype: DType](predicted: Tensor[dtype], target: Tensor[dtype]) -> Scalar[dtype]:
    """Mean Squared Error loss: mean((y_pred - y_true)^2)"""
    var diff = Tensor[dtype](predicted.shape)
    
    for i in range(predicted.size):
        var d = predicted.data[i] - target.data[i]
        diff.data[i] = d * d
    
    return mean(diff)

fn cross_entropy_loss[dtype: DType](predicted: Tensor[dtype], target: Tensor[dtype]) -> Scalar[dtype]:
    """Cross-entropy loss: -sum(y_true * log(y_pred))"""
    var loss = Scalar[dtype](0)
    
    for i in range(predicted.size):
        loss += -target.data[i] * log(predicted.data[i])
    
    return loss

# =============================================================================
# Main Demo
# =============================================================================

fn main():
    print("ðŸ”¥ MojoTensor - Neural Network Building Blocks\n")
    
    print("=" * 60)
    print("1. ACTIVATION FUNCTIONS")
    print("=" * 60)
    
    # Test ReLU
    var x = Tensor[DType.float32](5)
    x.data[0] = -2.0
    x.data[1] = -1.0
    x.data[2] = 0.0
    x.data[3] = 1.0
    x.data[4] = 2.0
    
    print("Input:", x)
    print("ReLU(x):", relu(x))
    print("Sigmoid(x):", sigmoid(x))
    print("Tanh(x):", tanh(x))
    print()
    
    # Test Softmax
    var logits = Tensor[DType.float32](3)
    logits.data[0] = 1.0
    logits.data[1] = 2.0
    logits.data[2] = 3.0
    
    print("Logits:", logits)
    var probs = softmax(logits)
    print("Softmax(logits):", probs)
    print("Sum of probabilities:", sum(probs))
    print()
    
    print("=" * 60)
    print("2. SIMPLE NEURAL NETWORK FORWARD PASS")
    print("=" * 60)
    
    # Input: 1 sample, 4 features
    var input = Tensor[DType.float32](1, 4)
    input.data[0] = 0.5
    input.data[1] = 1.0
    input.data[2] = -0.5
    input.data[3] = 2.0
    print("Input (1x4):", input)
    print()
    
    # Layer 1: 4 -> 8 (weights: 4x8)
    var w1 = Tensor[DType.float32](4, 8)
    w1.fill(0.1)
    print("Weights Layer 1 (4x8): initialized to 0.1")
    
    var z1 = matmul(input, w1)
    print("z1 = input @ w1:", z1)
    
    var a1 = relu(z1)
    print("a1 = ReLU(z1):", a1)
    print()
    
    # Layer 2: 8 -> 3 (output classes)
    var w2 = Tensor[DType.float32](8, 3)
    w2.fill(0.1)
    print("Weights Layer 2 (8x3): initialized to 0.1")
    
    var z2 = matmul(a1, w2)
    print("z2 = a1 @ w2:", z2)
    
    var output = softmax(z2)
    print("output = Softmax(z2):", output)
    print()
    
    print("=" * 60)
    print("3. LOSS CALCULATION")
    print("=" * 60)
    
    # Target (one-hot encoded)
    var target = Tensor[DType.float32](1, 3)
    target.data[0] = 0.0
    target.data[1] = 1.0
    target.data[2] = 0.0
    print("Target (one-hot):", target)
    
    var loss = cross_entropy_loss(output, target)
    print("Cross-Entropy Loss:", loss)
    print()
    
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print("âœ… Built activation functions (ReLU, Sigmoid, Tanh, Softmax)")
    print("âœ… Implemented loss functions (MSE, Cross-Entropy)")
    print("âœ… Demonstrated 2-layer neural network forward pass")
    print("âœ… All operations SIMD-accelerated!")
    print()
    print("ðŸš€ Ready to build full ML frameworks on top of MojoTensor!")
